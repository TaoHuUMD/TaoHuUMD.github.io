<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta name="robots" content="noindex,nofollow">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

<title>Tao Hu</title>

<style type="text/css">
    #div1{
      width: 100%;
      height: 100%;
      border:#000 solid 0px;
      margin: 50px auto;
      /* overflow: hidden; */
    }
    #div1 img{
      cursor: pointer;
      transition: all 0.6s;
    }
    #div1 img:hover{
      transform: scale(1.4);
    }

    #div1 iframe{
      cursor: pointer;
      transition: all 0.6s;
    }
    #div1 iframe:hover{
      transform: scale(1.4);
    }
    </style>

</head>
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable">
<tr>
<td> <a href="./"><img src="./files/ht.jpg" alt="" height="200px" /></a>&nbsp;</td>
<td align="left">
<p>
<font size="4">Tao Hu (胡涛)</font><br />
<br />

Postdoc Research Fellow, <br />

<a href="https://www.ntu.edu.sg/s-lab" target=&ldquo;blank&rdquo;>S-Lab</a>,
<a href="https://www.ntu.edu.sg/computing" target=&ldquo;blank&rdquo;>College of Computing and Data Science</a>, <a href="https://www.ntu.edu.sg/" target="_blank">Nanyang Technological University</a>, Singapore<br /> <br />

Ph.D., <a href="https://www.cs.umd.edu/" target=&ldquo;blank&rdquo;>Department of Computer Science</a>,  <a href="http://www.cs.umd.edu/" target="_blank">University of Maryland, College Park</a>, USA <br />
<br />

Email: tao.hu [at] ntu.edu.sg, taohu [at] umd.edu <br/>

<br />
<a href="files/Resume_TaoHu.pdf" target=&ldquo;blank&rdquo;>Resume/CV</a> | <a href="https://scholar.google.com/citations?user=lKB_cJAAAAAJ&hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a> | <a href="https://github.com/TaoHuUMD" target=&ldquo;blank&rdquo;>GitHub</a> | <a href="https://twitter.com/TaoHuThu" target=&ldquo;blank&rdquo;>Twitter</a> 

</p>
</td>
</tr></table>

<h2>Biography</h2>
<p>
    I am a Postdoc Research Fellow at NTU, working with Prof. <a href="https://liuziwei7.github.io/" target=&ldquo;blank&rdquo;> Ziwei Liu</a>, at S-Lab and MMLab@NTU directed by Prof. <a href="https://www.mmlab-ntu.com/person/ccloy/" target=&ldquo;blank&rdquo;> Chen Change Loy</a>. I completed my Ph.D. in Computer Science at the University of Maryland, College Park advised by Prof. <a href="https://www.cs.umd.edu/~zwicker/" target=&ldquo;blank&rdquo;>Matthias Zwicker</a> (CS Department Chair). During my PhD, I had the pleasure to visit the <a href="https://www.mpi-inf.mpg.de/home" target=&ldquo;blank&rdquo;> Max Planck Institute </a> in Saarbrücken, Germany under the guidance of  Prof. <a href="https://people.mpi-inf.mpg.de/~theobalt/" target=&ldquo;blank&rdquo;>Christian Theobalt</a> in 2020, and 3DV Lab at Tsinghua University, China under the guidance of Prof. <a href="https://liuyebin.com/" target=&ldquo;blank&rdquo;> Yebin Liu</a> in 2021. Before that, I received my B.Eng. and M.S. degree from the School of Software, Beijing Institute of Technology in 2015 and 2018 respectively. 
  </p>


<h2>Research Interest</h2>

My research focuses on 3D representation learning, 3D reconstruction, neural rendering, generative models, and human appearance/motion/geometry modeling.  

<h2>Research Experience</h2>
<table class="imgtable">
<tr>
    <td align="left">
            <img src="./files/ntu.jpg" alt="" height="40px" width="130px"/>
    </td>
    <td>
        <p><a href="https://www.ntu.edu.sg/computing" target=&ldquo;blank&rdquo;>College of Computing and Data Science</a>, Nanyang Technological University, Singapore </p>
        <p>Research Fellow, Jun. 2023 ~ present</p> 
        <p>Advisor: Prof. <a href="https://liuziwei7.github.io/" target=&ldquo;blank&rdquo;> Ziwei Liu</a> </br>
            Affiliated: <a href="https://www.ntu.edu.sg/s-lab" target=&ldquo;blank&rdquo;>S-Lab</a> and <a href="https://www.mmlab-ntu.com/index.html" target=&ldquo;blank&rdquo;>MMLab@NTU</a> directed by Prof. <a href="https://www.mmlab-ntu.com/person/ccloy/" target=&ldquo;blank&rdquo;> Chen Change Loy</a>.
        </p>         
    </td>
</tr>

<tr>
    <td align="left">
            <img src="./files/umd.png" alt="" width="130px"/>
    </td>
    <td>
        <p><a href="http://www.cs.umd.edu/" target="_blank" rel="noopener noreferrer">Department of Computer Science</a>, University of Maryland, College Park, USA </p>
        <p>Ph.D., Aug. 2018 ~ Jun. 2023</p> 
        <p>Supervisor: Prof. <a style="color:Purple" href="https://www.cs.umd.edu/~zwicker/" target="_blank">Matthias Zwicker</a></p>
    </td>
</tr>




<tr>
    <td align="left">
            <img src="./files/thu.png" alt="" width="120px"/>
    </td>
    <td>
        <p><a href="https://liuyebin.com/" target="_blank" rel="noopener noreferrer">3DV Lab</a>, Tsinghua University, Beijing, China </p>
        <p>Research Intern, Apr. 2021 ~ Nov. 2021</p> 
        <p>Supervisor: Prof. <a style="color:Purple" href="https://liuyebin.com/" target="_blank">Yebin Liu</a></p> 
    </td>
</tr>

<tr>
    <td align="left">
            <img src="./files/mpi2.jpg" alt="" width="120px"/>
    </td>
    <td>
        <p><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Graphics, Vision & Video Group</a>, Max Planck Institute for Informatics, Saarbrücken, Germany </p>
        <p>Research Intern, Mar. 2020 ~ Sep. 2020</p> 
        <p>Supervisor: Prof. <a href="https://people.mpi-inf.mpg.de/~theobalt/" target=&ldquo;blank&rdquo;>Christian Theobalt</a></p> 
    </td>
</tr>

<tr>
    <td align="left">
            <img src="./files/shanghai.jpeg" alt="" width="130px"/>
    </td>
    <td>
        <p><a href="https://www.shlab.org.cn/" target="_blank" rel="noopener noreferrer">Shanghai AI Lab</a>, Shanghai, China </p>
        <p>Research Intern, Apr. 2023 ~ Jun. 2023</p> 
        <p>Supervisor: Prof. <a href="https://liuziwei7.github.io/" target=&ldquo;blank&rdquo;> Ziwei Liu</a> </p> 
    </td>
</tr>

<tr>
    <td align="left">
            <img src="./files/bytedance.png" alt="" width="130px"/>
    </td>
    <td>
        <p><a href="https://www.bytedance.com/en" target="_blank" rel="noopener noreferrer">Intelligent Creation Lab</a>, ByteDance Inc USA, Remote </p>
        <p>Research Intern, Dec. 2021 ~ Jul. 2022</p> 
        <p>Supervisor: Dr. <a href="http://www-scf.usc.edu/~hongyixu/" target=&ldquo;blank&rdquo;>Hongyi Xu</a>, Dr. <a href="http://linjieluo.com/" target=&ldquo;blank&rdquo;>Linjie Luo</a>
        
            
        </p> 
    </td>
</tr>


<tr>
    <td align="left">
            <img src="./files/msra2.jpg" alt="" width="130px"/>
    </td>
    <td>
        <p><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank" rel="noopener noreferrer">Speech Group</a>, Microsoft Research Asia (MSRA), Beijing, China </p>
        <p>Research Intern, Jun. 2017 - Nov. 2017</p> 
        <p>Supervisor: Dr. <a href="https://www.microsoft.com/en-us/research/people/kaic/" target=&ldquo;blank&rdquo;>Kai Chen</a> 
        </p> 
    </td>
</tr>


</table>

<!-- Project -->
<a id="publications" class="anchor"></a>
<h2>Preprint </h2>
<table class="imgtable">
<!--  -->



    <tr>
        <td><img class="proj_thumb" src="./files/fashionengine.jpg" alt="" height="100px"/>&nbsp;</td>
        <td>
        <p class="pub_title"><a href="https://taohuumd.github.io/projects/FashionEngine" target=&ldquo;blank&rdquo;><b>FashionEngine: Interactive 3D Human Generation and Editing via Multimodal Controls.</b></a> </p>
        <p class="pub_author">
            <b>Tao Hu</b>, Fangzhou Hong, Zhaoxi Chen, Ziwei Liu.</br>
            arXiv:2404.01655, 2024<br/>
    <a href="https://taohuumd.github.io/projects/FashionEngine" target=&ldquo;blank&rdquo;> <font color="black">[Project Page]</font> </a> <a href="https://www.youtube.com/watch?v=kJ_nuzde5ko" target=&ldquo;blank&rdquo;> <font color="black">[Video]</font></a> <a href="https://arxiv.org/pdf/2404.01655.pdf" target=&ldquo;blank&rdquo;> <font color="black">[arXiv]</font> </a>   
    <br><font color="#993300"><b>&#8594;</b> The first work that constructs <b>an interactive 3D human generation and editing system with multimodal controls</b> (e.g., texts, images, hand-drawing sketches) in a unified framework. 
    </font>
    </p> </td>
   </tr>

<tr>
<td><img class="proj_thumb" src="./files/liff2.jpg" alt="" height="100px"/>&nbsp;</td>
<td>
<p class="pub_title">HumanLiff: Layer-wise 3D Human Generation with Diffusion Model.</p>
<p class="pub_author">Shoukang Hu, Fangzhou Hong, <b>Tao Hu </b>, Liang Pan, Weiye Xiao, Haiyi Mei, Lei Yang, Ziwei Liu<br>
<font color="gray">arXiv:2308.09712, 2023</font><br>
<a href= "https://arxiv.org/pdf/2308.09712.pdf" target="_blank"><font color="black">[Paper]</font></a>  
<a href="https://skhu101.github.io/HumanLiff/" target="_blank"><font color="black">[Project Page]</font></a>
<a href="https://github.com/skhu101/HumanLiff" target="_blank"><font color="black">[Code]</font></a>
<br><font color="#993300"><b>&#8594;</b> A diffusion-based approach for <b>layer-wise 3D human generation</b>.  
</p> </td>
</tr>

</table>


<h2>Selected Publications
<!-- <a style="color:Black" class="p1" href="https://scholar.google.com/citations?user=lKB_cJAAAAAJ&hl=en" target="_blank">
    <font size="2"> [Full List]</font></a> -->

</h2>

<table class="imgtable">

<!-- CVPR -->

<tr>
    <td><img class="proj_thumb" src="./files/structldm2.jpg" alt="" height="100px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://taohuumd.github.io/projects/StructLDM" target=&ldquo;blank&rdquo;><b>StructLDM: Structured Latent Diffusion for 3D Human Generation. </b></a> </p>
    <p class="pub_author">
        <b>Tao Hu</b>, Fangzhou Hong, Ziwei Liu.</br>
        European Conference on Computer Vision (ECCV 2024) </br>
    <a href="https://taohuumd.github.io/projects/StructLDM" target=&ldquo;blank&rdquo;> <font color="black">[Project Page]</font> </a> <a href="https://www.youtube.com/watch?v=cZ3PGqOcRqI" target=&ldquo;blank&rdquo;> <font color="black">[Video]</font> </a>
<a href="https://github.com/TaoHuUMD/StructLDM" target=&ldquo;blank&rdquo;> <font color="black">[Code]</font> </a>  <a href="https://arxiv.org/pdf/2404.01241.pdf" target=&ldquo;blank&rdquo;>      <font color="black">[arXiv]</font> </a> [<b><a href="https://x.com/dreamingtulpa/status/1776940745707933844" target=&ldquo;blank&rdquo;><font color="#116701">Media Coverage</font></a></b>]  
[<b><font color="#116701">Media Coverage in Chinese: <a href="https://mp.weixin.qq.com/s/1resmGLBW3s5kGHRAKFidw" target=&ldquo;blank&rdquo;><font color="#116701">1,</font></a> <a href="https://mp.weixin.qq.com/s/WYPl49OSaOaf4azcO4TgnA" target=&ldquo;blank&rdquo;><font color="#116701">2</font></a></font></b>]
    <br><font color="#993300"><b>&#8594;</b> <b>A new paradigm for 3D human generation </b> from 2D image collections, with 3 key designs: a structured 2D latent space, a structured auto-decoder, and a structured latent diffusion model.
    </font>        
    
    </p> </td>
    </tr>

<tr>
    <td><img class="proj_thumb" src="./files/surmo3.jpg" alt="" height="100px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://taohuumd.github.io/projects/SurMo/" target=&ldquo;blank&rdquo;>SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering.</b></a></br> </p>
    <p class="pub_author"><b>Tao Hu</b>, Fangzhou Hong, Ziwei Liu.</br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2024) </br>
    
    [<a href= "https://arxiv.org/pdf/2404.01225.pdf" target="_blank"><font color="black">Paper</font></a>]
    [<a href="https://TaoHuUMD.github.io/projects/SurMo/" target="_blank"><font color="black">Project Page</font></a>]
    [<a href="https://www.youtube.com/watch?v=m_rP5HwL53I" target="_blank"><font color="black">Video</font></a>] 
    [<a href="https://github.com/TaoHuUMD/SurMo" target="_blank"><font color="black">Code</font></a>]
    [<b><a href="https://mp.weixin.qq.com/s/QKuR2BzCFSBQ0epP2b4ZWQ" target=&ldquo;blank&rdquo;><font color="#116701">Media Coverage in Chinese:机器之心</font></a></b>]
    <br><font color="#993300"><b>&#8594;</b> <b>A new paradigm for learning dynamic human rendering</b> from videos by jointly modeling the temporal motion dynamics and human appearances in a unified framework based on <b>a novel surface-based triplane</b>.</font>     

    </p> </td>
    </tr>

<!-- tvcg https://ieeexplore.ieee.org/document/10190111-->
<tr>
    <td><img class="proj_thumb" src="./files/hvtrppimg.jpg" alt="" height="80px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://taohuumd.github.io/projects/hvtrpp/" target=&ldquo;blank&rdquo;> HVTR++: Image and Pose Driven Human Avatars using Hybrid Volumetric-Textural Rendering.</b></a></br> </p>
    <p class="pub_author"><b>Tao Hu</b>, Hongyi Xu, Linjie Luo, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, Matthias Zwicker.</br>
    IEEE Transactions on Visualization and Computer Graphics (TVCG 2023) <br/>
    [<a href= "https://1drv.ms/b/s!Att91f8pjJXNmz7zi95OqPloJL6E?e=fMzG8U" target="_blank"><font color="black">Paper</font></a>]
    [<a href="https://TaoHuUMD.github.io/projects/hvtrpp/" target="_blank"><font color="black">Project Page</font></a>] 
    [<a href="https://youtu.be/RdKLfRYtg3I" target="_blank"><font color="black">Video</font></a>]
    [<a href="https://github.com/TaoHuUMD/SurMo" target="_blank"><font color="black">Code</font></a>] 
    <br><font color="#993300"><b>&#8594;</b> <b>A virtual teleportation system </b> using sparse view cameras based on a novel texel-aligned multimodal representation.</font>
    </p> </td>
    </tr>
<!-- 3DV https://github.com/TaoHuUMD.github.io/projects/hvtr/ -->
<tr>
    <td><img class="proj_thumb" src="./files/hvtrimg2.jpg" alt="" height="110px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://TaoHuUMD.github.io/projects/hvtr/" target=&ldquo;blank&rdquo;>HVTR: Hybrid Volumetric-Textural Rendering for Human Avatars.</b></a></br> </p>
    <p class="pub_author"><b>Tao Hu</b>, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, Matthias Zwicker.</br>
    International Conference on 3D Vision (3DV 2022)  <br/>     
    [<a href= "https://arxiv.org/pdf/2112.10203.pdf" target="_blank"><font color="black">Paper</font></a>] 
    [<a href="https://TaoHuUMD.github.io/projects/hvtr/" target="_blank"><font color="black">Project Page</font></a>] [<a href="https://youtu.be/LE0-YpbLlkY?si=DfXp4vLKUVGCJlKG" target="_blank"><font color="black">Video</font></a>] <a href="files/HVTR_Poster.pdf" target=&ldquo;blank&rdquo;> <font color="black">[Poster]</font> </a> <a href="https://arxiv.org/abs/2111.12685" target=&ldquo;blank&rdquo;> <font color="black">[arXiv]</font> </a>
    [<a href="https://github.com/TaoHuUMD/SurMo" target="_blank"><font color="black">Code</font></a>] <br>
    <font color="#993300"><b>&#8594;</b> The first work that <b>combines classical volumetric rendering with probabilistic generative models</b> for efficient and realistic dynamic human rendering.</font>

    </p> </td>
    </tr>
    
<!-- ICCV -->
<tr>
    <td><img class="proj_thumb" src="./files/egorenderer.jpg" alt="" height="80px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://vcai.mpi-inf.mpg.de/projects/EgoRenderer/" target=&ldquo;blank&rdquo;>EgoRenderer: Rendering Human Avatars from Egocentric Camera Images.</b></a></br> </p>
    <p class="pub_author"><b>Tao Hu</b>, Kripasindhu Sarkar, Lingjie Liu, Matthias Zwicker, Christian Theobalt.</br>
    IEEE International Conference on Computer Vision (ICCV 2021)<br/>     
    
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_EgoRenderer_Rendering_Human_Avatars_From_Egocentric_Camera_Images_ICCV_2021_paper.pdf" target=&ldquo;blank&rdquo;> <font color="black">[Paper]</font> </a> <a href="https://vcai.mpi-inf.mpg.de/projects/EgoRenderer/" target=&ldquo;blank&rdquo;> <font color="black">[Project Page]</font> </a> <a href="https://www.youtube.com/watch?v=jrZfEEP_-zA" target=&ldquo;blank&rdquo;> <font color="black">[Video]</font> </a> <a href="files/EgoRenderer_Poster.pdf" target=&ldquo;blank&rdquo;> <font color="black">[Poster]</font> </a>  <a href="https://arxiv.org/abs/2111.12685" target=&ldquo;blank&rdquo;> <font color="black">[arXiv]</font> </a> </a> [<b><a href="https://www.technology.org/2021/11/27/egorenderer-rendering-human-avatars-from-egocentric-camera-images/" target=&ldquo;blank&rdquo;><font color="#116701">Media Coverage: SCIENCE & TECHNOLOGY NEWS</font></a></b>]  
    <br><font color="#993300"><b>&#8594;</b> <b>A mobile virtual teleportation system </b> integrating mobile motion capture and free-view rendering in a egocentric setup.</font>
    </p> </td>
    </tr>



<!-- WACV -->
<tr>
    <td><img class="proj_thumb" src="./files/wacv2.jpg" alt="" height="75"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://openaccess.thecvf.com/content/WACV2021/papers/Hu_Learning_to_Generate_Dense_Point_Clouds_With_Textures_on_Multiple_WACV_2021_paper.pdf" target=&ldquo;blank&rdquo;><b>Learning to Generate Dense Point Clouds with Textures on Multiple Categories.</b></a></br> </p>
    <p class="pub_author"><b>Tao Hu</b>, Geng Lin, Zhizhong Han, Matthias Zwicker.</br>
    IEEE Winter Conference on Applications of Computer Vision (WACV 2021)<br/> 
    
    <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Hu_Learning_to_Generate_Dense_Point_Clouds_With_Textures_on_Multiple_WACV_2021_paper.pdf" target=&ldquo;blank&rdquo;> <font color="black">[Paper]</font> </a> <a href="https://github.com/TaoHuUMD/3D-Reconstruction" target=&ldquo;blank&rdquo;> <font color="black">[Code]</font> </a> <a href="https://arxiv.org/abs/1912.10545" target=&ldquo;blank&rdquo;> <font color="black">[arXiv]</font> </a>          
    <br><font color="#993300"><b>&#8594;</b> Extend the multi-view representation for <b> generalizable geometry/texture reconstructions </b> from single RGB images. </font>    
    </p> </td>
    </tr>


<!-- AAAI -->
<tr>
    <td><img class="proj_thumb" src="./files/aaai_2020_pipe2.jpg" alt="" height="100px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6734" target=&ldquo;blank&rdquo;><b>3D Shape Completion with Multi-view Consistent Inference.</b></a></br> </p>
    <p class="pub_author"><b>Tao Hu</b>, Zhizhong Han, Matthias Zwicker.</br>
    AAAI Conference on Artificial Intelligence (AAAI 2020, Oral, top 10% among accepted papers in 3D vision track)</br>    
    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6734" target=&ldquo;blank&rdquo;> <font color="black">[Paper]</font> </a> <a href="https://github.com/TaoHuUMD/3D-Reconstruction" target=&ldquo;blank&rdquo;> <font color="black">[Code]</font> </a>  <a href="https://arxiv.org/abs/1911.12465" target=&ldquo;blank&rdquo;> <font color="black">[arXiv]</font> </a>    
    <br><font color="#993300"><b>&#8594;</b> Introduce a <b>self-supervised multi-view consistent inference technique</b> to enforce geometric consistency for multi-view representation.</font>    
    </p> </td>      
    </tr>



<!-- ICCVW -->
<tr>
    <td><img class="proj_thumb" src="./files/iccvw.jpg" alt="" height="75px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/GMDL/Hu_Render4Completion_Synthesizing_Multi-View_Depth_Maps_for_3D_Shape_Completion_ICCVW_2019_paper.pdf" target=&ldquo;blank&rdquo;><b>Render4Completion: Synthesizing Multi-view Depth Maps for 3D Shape Completion. </b></a> </p>
    <p class="pub_author">
        <b>Tao Hu</b>, Zhizhong Han, Abhinav Shrivastava, Matthias Zwicker.</br>
IEEE ICCV Geometry Meets Deep Learning Workshop (ICCVW 2019, Oral)<br/>
<a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/GMDL/Hu_Render4Completion_Synthesizing_Multi-View_Depth_Maps_for_3D_Shape_Completion_ICCVW_2019_paper.pdf" target=&ldquo;blank&rdquo;> <font color="black">[Paper]</font> </a> <a href="https://github.com/TaoHuUMD/3D-Reconstruction" target=&ldquo;blank&rdquo;> <font color="black">[Code]</font> </a>  <a href="https://arxiv.org/abs/1904.08366" target=&ldquo;blank&rdquo;> <font color="black">[arXiv]</font> </a>
    <br><font color="#993300"><b>&#8594;</b> Present <b>multi-view based 3D shape representation</b>  with a multi-view completion net for dense 3D shape completion.</font>    
    </p> </td>
    </tr>


    <!-- cryengine #993300 -->
<tr>
    <td><img class="proj_thumb" src="./files/plogin2.jpg" alt="" height="100px"/>&nbsp;</td>
    <td>
    <p class="pub_title"><a href="http://highlights.paper.edu.cn/index/paper_detail/5983" target=&ldquo;blank&rdquo;><b>A Parallel Video Player Plugin for CryEngine. </b></a> </p>
    <p class="pub_author">
        <b>Tao Hu</b>, Gangyi Ding, Lijie Li, Longfei Zhang.</br>
        Highlights of Sciencepaper, Chinese Journal, May 2016. <br/>
        Software Copyright (2016SR010412) <a href="http://highlights.paper.edu.cn/index/paper_detail/5983" target=&ldquo;blank&rdquo;> <font color="black">[Paper]</font> </a>
        
        <br><font color="#993300

        "><b>&#8594;</b> Propose <b> a parallel video player plugin </b> for CryEngine3 for a speedup from 16 FPS to 54 FPS at a large-scale virtual stage with 40 LED screens playing videos simultaneously for <b>digital performance</b>. 
        </font>
    </p> </td>
    </tr>

</table>




<!-- <p style="line-height:180%"> Services CVPR, ICCV, ECCV, NeurIPS, ICLR, 3DV, WACV, ICPR, ACCV  </br> -->
<a id="services" class="anchor"></a>
<h2>Services</h2>

    Conference Reviewer: 
    <li>Conference on Computer Vision and Pattern Recognition (CVPR) </li> 
    <li>International Conference on Computer Vision (ICCV) </li>
    <li>European Conference on Computer Vision (ECCV) </li>
    <li>International Conference on 3D Vision (3DV) </li>
    <li>Winter Conference on Applications of Computer Vision (WACV) </li>
    <li>Conference on Neural Information Processing Systems (NeurIPS) </li>
    <li>International Conference on Learning Representations (ICLR) </li>
    <li>Asian Conference on Computer Vision (ACCV) </li>
    <li>International Conference on Pattern Recognition (ICPR) </li>
</b>
    Journal Reviewer:
    <li>Computer Graphics Forum </li>
    <li>Computer Vision and Image Understanding </li>
    <li>Image and Vision Computing</li>
    <li>Pattern Recognition Letters</li>


<h2> Selected Awards & Honors </h2> <!-- Selected Honors &amp; </p> -->
<p style="line-height:180%">
<b>Graduate National Scholarship (Top 2%), </b> Ministry of Education of China&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016 </br>
<b>Undergraduate National Scholarship (Top 2%), </b> Ministry of Education of China&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014</br>
</p>


<h2>Teaching Experience</h2>

Teaching Assistant, Dept. of Computer Science, UMD.
	<li>  CMSC425 Game Programming (Prof. Roger Eastman),  Fall 2019 </li>
    <li> CMSC425 Game Programming (Prof. Roger Eastman),  Spring 2019 </li>

    <li> CMSC 216 Introduction to Computer Systems (Mr. Laurence Herman), Fall 2018 </li>


<div id="footer">
<div id="footer-text">
<!--
    <font size="2"> 
All Rights Reserved. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

</div>
</div>

</body>
</html>
